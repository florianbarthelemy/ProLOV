\todo{we have to extend this Related work}

So far, Linked Data principles and practices are being adopted by an increasing number of data providers, getting as result a global data space on the Web containing hundreds of LOD datasets \cite{Heath_Bizer_2011}. There are already several guidelines for generating, publishing, interlinking, and consuming Linked Data \cite{Heath_Bizer_2011}. An important task, within the generation process, is to build the vocabulary to be used for modelling the domain of the data sources, and the common recommendation is to reuse as much as possible available vocabularies \cite{Heath_Bizer_2011,hyland14}. This reuse approach speeds up the vocabulary development, and therefore, publishers will save time, efforts, and resources. 

There are research efforts, like the NeOn Methodology \cite{suarezfigueroa2012ontology}, the Best Practices for Publishing Linked Data - W3C Working Group Note \cite{hyland14}, and the work proposed by Lonsdale et al. \cite{Lonsdale2010318}. However, at the time of writing we have not found specific and detailed guidelines that describe how to reuse available vocabularies at fine granularity level,i.e., reusing specific classes and properties. Our claim is that this difficulty in how to reuse vocabularies at low fine grained level is one of major barriers to the reuse of vocabularies on the Web and in consequence to deployment of Linked Data.


Moreover, the recent success of Linked Open Vocabularies (LOV\footnote{\url{http://lov.okfn.org/dataset/lov/}}) as a central point for curated catalog of ontologies is helping to convey on best practices to publish vocabularies on the Web, as well as to help in the Data publication activity on the Web. LOV comes with many features, such as an API, a search function and a SPARQL endpoint.

In this paper we propose an initial set of guidelines for this task, and provide technological support by means of a plugin for \protege, which is one of the popular frameworks for developing ontologies in a variety of formats including OWL, RDF(S), and XML Schema. It is backed by a strong community of developers and users in many domains. One success on \protege also depends on the availability to extend the core framework adding new functionalities by means of plug-ins. In addition, we propose to explore, design and implement a plug-in of LOV in \protege for easing the development of ontologies by reusing existing vocabularies at low fine grained level. The tool helps to improve the modeling and reuse of ontologies used in the LOD cloud.

%, by providing the following features in Prot{\'e}g{\'e}:
%\begin{itemize}
%\item Import easily vocabularies from LOV into \protege. 
%\item Propose to the user a list of candidate vocabularies in LOV matching the term
%\item Have an updating mechanism (synchronization) to LOV catalog
%\item Check if a new vocabulary created in \protege satisfied the LOV recommendations \cite{pybernard12}
%\item Suggest to LOV a new created vocabulary within \protege.
%\end{itemize}

%This paper presents \protege, a first implementation of the LOV realized as a plugin for the ontology editor \protege. 

\section{Reusing vocabulary terms\\ when building ontologies}\label{sec:reuse}

\todo{We have to improve this section- add a picture for workflow}
In the literature there exist many attempts to advise vocabulary publishers on the importance of reusing terms, as indicated in \todo{cite papers}. However, to the best of our knowledge there are not guidelines to help vocabulary practitioners to reuse vocabularies in real-world scenario. 

In this section we describe the workflow of reusing available vocabulary terms when building ontologies. In a nutshell, the task of building vocabularies by reusing available vocabulary terms consists of:

\begin{itemize}
	\item Searching for suitable vocabulary terms to reuse from the any vocabulary repository, such as BioPortal, LOV, Biotec.org, etc. The search should be conducted by using the terms of the application domain.
	\item Assessing the set of candidate terms from the vocabulary repository. In the particular case of LOV, the results include a score related to their ``importance'' in the corpus for each term retrieved.
	\item Selecting the most appropriate term taking into the account its score from the candidates terms proposed. Other criteria can be also considered here: (i) the stability of the URI namespace, (i) the 
	\item Including the selected term in the ontology that has being developed. The selected term will be the external term. There are three alternatives in this case: 
	\begin{itemize}
		\item Include the external term and use it directly in the local ontology by defining local axioms to or from that term in the local ontology.
		\item Include the external term, create a local term, and define the {\tt rdfs:subClassOf} or {\tt rdfs:subPropertyOf} axiom to relate both terms.
		\item Include the external term, create a local term, and define the {\tt owl:equivalentClass} or {\tt owl:equivalentProperty} axiom to relate both terms. 				
	\end{itemize}
\end{itemize}

\subsection{Use case scenario: lobid vocabulary}
\label{lobid vocabulary}
The lobid vocabulary\footnote{\url{http://purl.org/lobid/lv}} is a vocabulary designed for the  linking open bibliographic data services. The ontology was first published on 2012-03-02 with only two properties, a minimal metadata information and labels in English. Since then, there have been 15 different versions and the current version (version of 2015-02-09) of the ontology contains 8 classes and 16 properties. Based on the different changes occurred during the on-going development of the lobid vocabulary, vocabulary changes can be grouped in two categories:
\begin{itemize}
\item Editorial changes (EDc) are the ones related with labels and comments translations, typos fixed. Those changes don't affect the structure of the vocabulary.
\item Semantic changes (SMc), are related with modifying the structure of the vocabulary, by either adding new axioms, new classes and properties.
\end{itemize}

Furthermore, the semantic changes can be broken down in four categories related to the main parts of a vocabulary, that are metadata, classes, properties and axioms. Thus we can group them as follows:

\begin{itemize}
\item Metadata changes (MTc), when the changes concern the metadata section of the vocabulary
\item Property changes (PPc) 
\item Classes changes (CLc) 
\item Axioms changes (AXc) for changes on general axioms of the vocabulary

\end{itemize}
  


